{"cells":[{"cell_type":"markdown","metadata":{"id":"aWjQ5MigqRZv"},"source":["# LightGlue Video Workflow\n","In this notebook we\n","- Load a video and seperate it into frames\n","- match two pairs of images using LightGlue with early stopping and point pruning."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":129924,"status":"ok","timestamp":1740756951478,"user":{"displayName":"Johannes Kähler","userId":"16147281443660370541"},"user_tz":-60},"id":"miW8RB01qRZ_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"06914902-4114-429c-c53b-f1d819d5d6c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/LightGlue\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building editable for lightglue (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]},{"output_type":"stream","name":"stderr","text":["/content/LightGlue/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n","  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n"]},{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n","\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.org (3.171.85.15)] [Con\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","\r                                                                                                    \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","\r0% [2 InRelease 82.2 kB/128 kB 64%] [Connected to cloud.r-project.org (3.171.85.15)] [Connected to r\r                                                                                                    \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,649 kB]\n","Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,688 kB]\n","Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,830 kB]\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,235 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,957 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,533 kB]\n","Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,321 kB]\n","Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,708 kB]\n","Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,664 kB]\n","Fetched 29.0 MB in 4s (7,343 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","E: Command line option --progress-bar is not understood in combination with the other options\n","\n","\n"]}],"source":["# If we are on colab: this clones the repo and installs the dependencies\n","from pathlib import Path\n","\n","if Path.cwd().name != \"LightGlue\":\n","    !git clone --quiet https://github.com/cvg/LightGlue/\n","    %cd LightGlue\n","    !pip install --progress-bar off --quiet -e .\n","\n","from lightglue import LightGlue, SuperPoint, DISK\n","from lightglue.utils import load_image, rbd\n","from lightglue import viz2d\n","import torch\n","\n","# Installiere FFmpeg\n","!apt-get update\n","!apt-get install -y ffmpeg --progress-bar off --quiet\n","print(f\"\\n\")\n","\n","torch.set_grad_enabled(False)\n","images = Path(\"assets\")"]},{"cell_type":"markdown","source":["## Load OpenCV to preprocess the video into frames"],"metadata":{"id":"KT4IGoX_x9iV"}},{"cell_type":"code","source":["import cv2\n","import os\n","import numpy\n","\n","\n","input_folder = \"/content/drive/MyDrive/Promotion/Code/feature_matching/LightGlue-main/data/\"\n","output_folder = \"/content/drive/MyDrive/Promotion/Code/feature_matching/LightGlue-main/frames/\"\n","video_path = input_folder + \"Panama_City_topdown.mp4\"\n","os.makedirs(output_folder, exist_ok=True)\n","\n","!ffmpeg -i /content/drive/MyDrive/Promotion/Code/feature_matching/LightGlue-main/data/Panama_City_topdown.mp4\n","\n","# Funktion zum Extrahieren der Frames aus dem Video\n","def extract_frames(video_path):\n","    video = cv2.VideoCapture(video_path)\n","\n","    # Überprüfe, ob das Video erfolgreich geladen wurde\n","    if not video.isOpened():\n","        print(\"Fehler: Das Video konnte nicht geladen werden.\")\n","    else:\n","        print(\"Das Video wurde erfolgreich geladen.\")\n","        print(video)\n","\n","        frames = []\n","        while True:\n","            ret, frame = video.read()\n","            print(frame, ret)\n","            if not ret:\n","               break\n","            frames.append(frame)\n","        video.release()\n","        return frames\n","\n","# Beispiel: Feature Matching auf den ersten beiden Frames\n","def feature_matching_with_lightglue(video_path):\n","    # 1. Extrahiere die Frames aus dem MP4-Video\n","    frames = []\n","    frames = extract_frames(video_path)\n","\n","    # Stelle sicher, dass genügend Frames vorhanden sind\n","    if len(frames) < 2:\n","        print(\"Das Video enthält nicht genug Frames.\")\n","        return\n","\n","    # 2. Verwende LightGlue für das Feature Matching\n","    frame1 = frames[0]  # Erster Frame\n","    frame2 = frames[1]  # Zweiter Frame\n","\n","    # Wandle die Bilder in Graustufen um, da LightGlue nur Graustufenbilder verarbeitet\n","    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n","    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n","\n","    # 3. Feature Matching mit LightGlue\n","    matches, keypoints1, keypoints2 = lightglue.match(gray1, gray2)\n","\n","    # 4. Visualisiere die Matches\n","    result_img = cv2.drawMatches(frame1, keypoints1, frame2, keypoints2, matches, None)\n","\n","    # Zeige das Ergebnis\n","    cv2.imshow(\"Feature Matches\", result_img)\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n","\n","\n","# Feature Matching auf den ersten beiden Frames durchführen\n","feature_matching_with_lightglue(video_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":543},"id":"n3CYg66pyE-w","executionInfo":{"status":"error","timestamp":1740755422632,"user_tz":-60,"elapsed":597,"user":{"displayName":"Johannes Kähler","userId":"16147281443660370541"}},"outputId":"a031cfd3-3408-4e1b-8165-ec9bcffb27e5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n","  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n","  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n","  libavutil      56. 70.100 / 56. 70.100\n","  libavcodec     58.134.100 / 58.134.100\n","  libavformat    58. 76.100 / 58. 76.100\n","  libavdevice    58. 13.100 / 58. 13.100\n","  libavfilter     7.110.100 /  7.110.100\n","  libswscale      5.  9.100 /  5.  9.100\n","  libswresample   3.  9.100 /  3.  9.100\n","  libpostproc    55.  9.100 / 55.  9.100\n","\u001b[1;31m/content/drive/MyDrive/Promotion/Code/feature_matching/LightGlue-main/data/Panama_City_topdown.mp4: No such file or directory\n","\u001b[0mFehler: Das Video konnte nicht geladen werden.\n"]},{"output_type":"error","ename":"TypeError","evalue":"object of type 'NoneType' has no len()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-32bca87fee4f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Feature Matching auf den ersten beiden Frames durchführen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mfeature_matching_with_lightglue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-32bca87fee4f>\u001b[0m in \u001b[0;36mfeature_matching_with_lightglue\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Stelle sicher, dass genügend Frames vorhanden sind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Das Video enthält nicht genug Frames.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}]},{"cell_type":"markdown","metadata":{"id":"pM--hqpBqRaC"},"source":["## Load extractor and matcher module\n","In this example we use SuperPoint features combined with LightGlue."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1740732971902,"user":{"displayName":"Johannes Kähler","userId":"16147281443660370541"},"user_tz":-60},"id":"ZQI3D02fqRaD"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 'mps', 'cpu'\n","\n","extractor = SuperPoint(max_num_keypoints=2048).eval().to(device)  # load the extractor\n","matcher = LightGlue(features=\"superpoint\").eval().to(device)"]},{"cell_type":"code","source":["def load_image(path):\n","    img = cv2.imread(path)\n","    img = cv2.resize(img, (640, 480)) #Größe anpassen, falls nötig\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #Farbschema anpassen, falls nötig\n","    img = torch.tensor(img).unsqueeze(0)unsqueeze(0).to(device).float() / 255\n","    return img"],"metadata":{"id":"DUv8YDQFzOcQ","executionInfo":{"status":"aborted","timestamp":1740732971903,"user_tz":-60,"elapsed":12,"user":{"displayName":"Johannes Kähler","userId":"16147281443660370541"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["frames = sorted(os.listdir(output_folder))\n","prev_img = load_image(os.path.join(output_folder, frames[0]))\n","prev_feats = extractor.extract(prev_img)"],"metadata":{"id":"7fEh4w4-0rHo","executionInfo":{"status":"aborted","timestamp":1740732971903,"user_tz":-60,"elapsed":11,"user":{"displayName":"Johannes Kähler","userId":"16147281443660370541"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range (1, len(frames)):\n","  curr_img = load_image(os.path.join(output_folder, frames[i]))\n","  curr_feats = extractor.extract(curr_img)\n","  matches = matcher({\"image0\": prev_feats, \"image1\": curr_feats})\n","  print(f\"Frame {i-1} -> {i}: {len(matches['matches'])} Matches\")\n","\n","  prev_img, prev_feats = curr_img, curr_feats"],"metadata":{"id":"WEd0n2W004f6","executionInfo":{"status":"aborted","timestamp":1740732971903,"user_tz":-60,"elapsed":11,"user":{"displayName":"Johannes Kähler","userId":"16147281443660370541"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualizing the results"],"metadata":{"id":"kZA3XgUy1x8Y"}},{"cell_type":"code","source":["# LightGlue returns keypoints as tensor\n","kp1 = [cv2.KeyPoint(x,y,1) for x, y in feats1['keypoints'].cpu().numpy()]\n","kp2 = [cv2.KeyPoint(x,y,1) for x, y in feats2['keypoints'].cpu().numpy()]\n","\n","# Transforming Matches\n","matches_list = matches['matches'].cpu().numpy()\n","good_matches = [cv2.DMatch(_queryIdx=m[0], _trainIdx=m[1], _distance=0) for m in matches_list]\n","\n","# Drawing Matches\n","img_matches = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n","\n","# Display\n","cv2.imshow(\"Feature Matches\", img_matches)\n","cv2.waitKey(0)\n","cv2.destroyAllWindows()"],"metadata":{"id":"DH8jHFdQ131T","executionInfo":{"status":"aborted","timestamp":1740732971903,"user_tz":-60,"elapsed":11,"user":{"displayName":"Johannes Kähler","userId":"16147281443660370541"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"fgmIK2x_3TGU"}},{"cell_type":"markdown","source":[],"metadata":{"id":"YnBqb_-X3Tj7"}},{"cell_type":"markdown","metadata":{"id":"FqVs3qt1qRaF"},"source":["## Easy example\n","The top image shows the matches, while the bottom image shows the point pruning across layers. In this case, LightGlue prunes a few points with occlusions, but is able to stop the context aggregation after 4/9 layers."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1740732971903,"user":{"displayName":"Johannes Kähler","userId":"16147281443660370541"},"user_tz":-60},"id":"_AtuHMQiqRaG"},"outputs":[],"source":["image0 = load_image(\"/content/LightGlue/assets/leaf_on.png\")    #\"Tahoua_Airport_01\")#images / \"DSC_0411.JPG\")\n","image1 = load_image(\"/content/LightGlue/assets/leaf_off.png\")    #images / \"Tahoua_Airport_02\")#images / \"DSC_0410.JPG\")\n","\n","feats0 = extractor.extract(image0.to(device))\n","feats1 = extractor.extract(image1.to(device))\n","matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","feats0, feats1, matches01 = [\n","    rbd(x) for x in [feats0, feats1, matches01]\n","]  # remove batch dimension\n","\n","kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","\n"]},{"cell_type":"code","source":["axes = viz2d.plot_images([image0, image1])\n","viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n","viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers', fs=20)\n","\n","kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n","viz2d.plot_images([image0, image1])\n","viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=10)"],"metadata":{"id":"KUDJXvQSwb7P","executionInfo":{"status":"aborted","timestamp":1740732971903,"user_tz":-60,"elapsed":11,"user":{"displayName":"Johannes Kähler","userId":"16147281443660370541"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZpJeHJ5tlfO","executionInfo":{"status":"aborted","timestamp":1740732971903,"user_tz":-60,"elapsed":10,"user":{"displayName":"Johannes Kähler","userId":"16147281443660370541"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"KedWcTtiqRaH"},"source":["## Difficult example\n","For pairs with significant viewpoint- and illumination changes, LightGlue can exclude a lot of points early in the matching process (red points), which significantly reduces the inference time."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1740732971903,"user":{"displayName":"Johannes Kähler","userId":"16147281443660370541"},"user_tz":-60},"id":"6DD52xZeqRaI"},"outputs":[],"source":["image0 = load_image(images / \"sacre_coeur1.jpg\")\n","image1 = load_image(images / \"sacre_coeur2.jpg\")\n","\n","feats0 = extractor.extract(image0.to(device))\n","feats1 = extractor.extract(image1.to(device))\n","matches01 = matcher({\"image0\": feats0, \"image1\": feats1})\n","feats0, feats1, matches01 = [\n","    rbd(x) for x in [feats0, feats1, matches01]\n","]  # remove batch dimension\n","\n","kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n","m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n","\n","axes = viz2d.plot_images([image0, image1])\n","viz2d.plot_matches(m_kpts0, m_kpts1, color=\"lime\", lw=0.2)\n","viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers')\n","\n","kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n","viz2d.plot_images([image0, image1])\n","viz2d.plot_keypoints([kpts0, kpts1], colors=[kpc0, kpc1], ps=6)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat":4,"nbformat_minor":0}